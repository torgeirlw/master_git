{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be01ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260f362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a152aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                transforms.Normalize((0.5), (0.5))\n",
    "                               ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n",
    "testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c46afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Examine a sample\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565d1c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd7b83aa90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap = 'Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257c7bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[1].numpy().squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbde0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "542a6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "portion_size = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70730c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = random.randint(0, images[1].shape[1]-portion_size[0]-1)\n",
    "y1 = random.randint(0, images[1].shape[2]-portion_size[1]-1)\n",
    "\n",
    "#x2, y2 = x1+portion_size[0]-1, y1+portion_size[1]-1\n",
    "x2, y2 = x1+portion_size[0], y1+portion_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fe923",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_im = images[1].numpy().squeeze()[x1:x2,y1:y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2,y1,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(part_im, cmap = 'Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a52917",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_im = part_im.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e688e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_im[:95].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90642e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ec6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668ea10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc2a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b26913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataloader that can sample part of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e08b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make custum gaussian mixture NLL loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4546db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_imgs = torch.load('MNIST_data/FashionMNIST/processed/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130300b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08705077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNeihgborhoodData(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, transform=None, nbh_size=(4,9)):#csv_file,root,\n",
    "        #self.root = root\n",
    "        self.image_dir = image_dir\n",
    "        #self.image_files = os.listdir(image_dir)\n",
    "        #self.data = pd.read_csv(csv_file).iloc[:, 1]\n",
    "        self.fashion_data = torch.load(image_dir)\n",
    "        self.fashion_imgs = self.fashion_data[0]/255\n",
    "        self.fashion_labels = self.fashion_data[1]\n",
    "        self.transform = transform\n",
    "        self.nbh_size = nbh_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.fashion_labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #image_name = os.path.join(self.image_dir, self.image_files[index])  \n",
    "        #image = PIL.Image.open(image_name)\n",
    "        image = self.fashion_imgs[index]\n",
    "        #label = self.data[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        nbh_im = torch.empty((0,32)) \n",
    "        for i in range(10):\n",
    "            x1 = np.random.randint(0, images[1].shape[1]-self.nbh_size[0]-1)\n",
    "            y1 = np.random.randint(0, images[1].shape[2]-self.nbh_size[1]-1)\n",
    "        \n",
    "            x2, y2 = x1+portion_size[0], y1+portion_size[1]\n",
    "            nbh = torch.flatten(image[x1:x2,y1:y2])\n",
    "            nbh = nbh[:32]\n",
    "            nbh_im = torch.cat((nbh_im, nbh.unsqueeze(0)), dim = 0)\n",
    "        \n",
    "        return nbh_im#image #, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac988f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussMixNLLLoss():\n",
    "    def __init__(self):\n",
    "        tntn\n",
    "def GaussianMixNLLLoss(input: Tensor,\n",
    "    target: Tensor,\n",
    "    var: Tensor,\n",
    "    full: bool = False,\n",
    "    eps: float = 1e-6,\n",
    "    reduction: str = \"mean\",\n",
    ") -> Tensor:\n",
    "    \n",
    "    if torch.any(var < 0):\n",
    "        raise ValueError(\"var has negative entry/entries\")\n",
    "        \n",
    "    # Clamp for stability\n",
    "    var = var.clone()\n",
    "    with torch.no_grad():\n",
    "        var.clamp_(min=eps)\n",
    "    \n",
    "    loss = 0.5 * (torch.log(var) + (input - target)**2 / var)\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "    \n",
    "def GaussianMixNLLLoss(pi_s, mu, var, target):\n",
    "    GaussianNLLLoss = torch.nn.GaussianNLLLoss()\n",
    "    total_likelihood = 0\n",
    "\n",
    "    # Calculate and sum the likelihoods of each component\n",
    "    for k, pi_k in enumerate(pi_s):\n",
    "        loss = GaussianNLLLoss(mu[k], target, var[k])\n",
    "        likelihood = torch.exp(-loss)\n",
    "        total_likelihood += pi_k * likelihood\n",
    "\n",
    "    # Take the negative log to give the GMM negative log-likelihood loss\n",
    "    GMM_NLLLoss = -torch.log(total_likelihood) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b281841",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = FashionNeihgborhoodData(image_dir='MNIST_data/FashionMNIST/processed/test.pt')#, transform=transforms.ToTensor())\n",
    "#fashion[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bed8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):#define neighborhood size\n",
    "    '''\n",
    "    Multilayer Perceptron.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(31, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2),#,\n",
    "        #nn.ReLU()\n",
    "        #nn.Sigmoid()\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        out = self.layers(x)\n",
    "        \n",
    "        mu = torch.reshape(out[:,0],(out.shape[0],1))\n",
    "        sig = torch.reshape(torch.exp(out[:,1]),(out.shape[0],1))\n",
    "\n",
    "        out = torch.cat((mu,sig),dim=1)\n",
    "        #print(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(fashion, batch_size=2,\n",
    "                        shuffle=True)#, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.GaussianNLLLoss(full=False, reduction='mean')\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=1e-4)\n",
    "#trainloader = torch.utils.data.DataLoader(train_dat, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e061bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20 #20\n",
    "loss_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs): # 5 epochs at maximum\n",
    "    \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {e+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "      \n",
    "      # Get and prepare inputs\n",
    "        data = torch.reshape(data,(-1,32))\n",
    "        inputs, targets = data[:,:-1], data[:,-1]\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "      \n",
    "      # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "      # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        mu_s = outputs[:,0]\n",
    "        sigma_s = outputs[:,1]  #sigma2 \n",
    "        #sigma_s = torch.exp(outputs[:,1])\n",
    "        #print(outputs.shape)\n",
    "      \n",
    "      # Compute loss\n",
    "        #loss = loss_function(outputs, targets)\n",
    "        loss = loss_function(mu_s, targets, sigma_s)\n",
    "      \n",
    "      # Perform backward pass\n",
    "        loss.backward()\n",
    "      \n",
    "      # Perform optimization\n",
    "        optimizer.step()\n",
    "      \n",
    "      # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        #if i % 500 == 0:\n",
    "        if i % 20 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "  # Process is complete.\n",
    "    loss_vals.append(current_loss)\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911952f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b67b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install pytorch torchvision torchaudio -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee240ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(dataloader, 0):\n",
    "    #print(batch_idx)\n",
    "    #print(torch.reshape(data,(-1,32)).shape)\n",
    "    data = torch.reshape(data,(-1,32))\n",
    "    inputs, targets = data[:,:-1], data[:,-1]\n",
    "    #inputs, targets = inputs.float(), targets.float()\n",
    "    #print(targets.shape)\n",
    "    targets = targets.reshape((targets.shape[0], 1))\n",
    "    #print(targets.shape)\n",
    "    \n",
    "    #print(data[:,:,])\n",
    "    inp = inputs\n",
    "    print(inputs.dtype)\n",
    "    break\n",
    "    #outputs = mlp(inputs)\n",
    "    #mu_s = outputs[:,0]\n",
    "    #sigma_s = outputs[:,1] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf005e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmmm[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp-0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf21da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68540b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
